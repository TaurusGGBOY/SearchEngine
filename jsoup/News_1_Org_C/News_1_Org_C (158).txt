一直以来，对架构这个词不知道怎么表述，似乎就是指一些MVC，前后分离，读写分离等等这些概念的集成，这些似乎也没错，但是不够准确。李的定义是 ”软件架构指软件系统的顶层架构“ ，详细讲就是1. 系统是一群关联个体的组成。2. 个体需要按照某种规则运行，架构需要明确个体运行和协作规则。 我原来的架构定义属于比较含糊，把不同层级的东西放在一块，结果怎么也讲述不清，每个层级有每个层级的架构。李的定义则抽象和清晰。 所谓银弹是指能大规模提高软件工程生产力的技术或方法。因为软件的复杂度是越来越高，原先为解决这些复杂度的方法终究会失效，这些方法的主要思想就是模块化，组件化，只不过是粒度越来越粗，但软件开发本身具有复杂性，不可见性和可变性，这些技术都不能根本的解决这些问题。 以在线广告投放为例 性能：平日流量总计在100亿左右，高峰期如双十一等则流量会翻好几倍。 平日平均12w qps/s， 按照峰值是均值的3倍，则峰值流量在36w qps/s, 而双十一又是这个数量的数倍，并且，adx对延迟容忍度非常低，要求响应在100ms以内，性能要求非常高，好消息是1. adx与rtb之间的连接是长连接，2. 来的流量有一部分不是我们想要的量，可以通过简单规则过滤。 可扩展性：因为成本问题，在高峰时需要简单扩容机器就可以承受压力。 高可用：短暂的停机重启(<15 分钟)可以忍受, 时间长了当天的投放任务可能完不成。 安全性：涉及隐私的个人信息很少。 成本：1.量很大，单机性能要高，2. 高峰低谷差距很大，要尽可能合理安排机器。 系统的复杂度体现在性能和扩展性方面，尤其是性能。 我所在业务（RTB）两种方式都有采取，单机：内存计算，基本不涉及磁盘，redis，httpcomponents reactor，多线程，线程池，异步，消息队列，调用子系统少（只有fc）。集群：F5负载均衡到多台机器 高可用更复杂，高性能通过拆分优化+堆机器都能比较好的解决，而高可用则是机器越多越复杂，环境因素更大，不可控因素更多。 接口，虚类，设计模式等等。对于可预测的变化和差异较小的变化可以解决，要多写不少代码，对于大的变化无解。 我所在业务复杂度主要来源是高性能和规模。高达几十万的qps/s是问题的主要来源，还有就是积累的数据量（用户信息，网页信息）越来越大是离线处理和在线分析的复杂度来源。 架构设计的三原则，分别是合适优于业界领先、简单优于复杂、演化优于一步到位。 应该是合适优于业界领先> 简单优于复杂>演化优于一步到位。选择架构，最重要的是基于业务，所以合适是最重要的，如果业务复杂或要求高，简单的架构不能承载业务就必须选复杂的架构，演化优于一步到位说的也是适合业务优先，而不是追求一步到位。 以美团的技术架构为例 初期: 简单优于复杂，业务刚开展，只要能完成业务就行 业务继续发展： 演化优于一步到位，根据业务发展来优化 2011年加入移动端： 合适优于业界领先 目前的架构： 演化优于一步到位 见问题3，复杂性体现在高性能流量处理，高可扩展性。 消息队列的复杂性主要体现在这几个方面：高性能消息读取、高可用消息写入、高可用消息存储、高可用消息读取。设计TPS为1380，QPS为13800 消息队列一般具有时间特性，把近期的数据缓存在redis中提高读性能，单个redis进程读都能达到3w qps/s，为保证数据安全，在写redis之前将数据写入mysql。另，单一redis list不能支撑1380的写入，可以将同一消息队列中消息分散写入多个list。 由此可见，RocketMQ主要特性是支持事务性消息，在某些需要强一致性的业务中（如支付等），跨系统的事务性有非常强烈的需求，所以阿里需要开发RocketMQ。 只懂概念，不懂实现 实际上很普通的技术一定要安上一个高大上的名词 追求“高大全”，集群，高可用，分布式满天飞 具有销售的潜力，选择了错误的职业 读写分离一般应用于读多写少的地方，如新闻，论坛等等，业务的规模瓶颈一般在于写的性能方面，因为读不够了可以再添加slave，甚至缓存，写的瓶颈单机一般在单机1000/s到5000/s，另外假如单表数据量太多（>1000w）或太大（100G），即使有索引，读性能也开始下降。 并不是性能不够就开始分库分表，而是单表数据太多或太大的时候开始考虑分库分表。性能不够的原因有很多，只有找不到其他方法优化性能的时候才考虑分库分表，因为分库分表引入的复杂度是很大的。 在肯定场合特别是互联网企业的业务中，NoSQL有很大的发挥余地，但是在要求事务性，稳定性的业务中，关系数据库仍然必不可少。 没遇见过。 连接不会太多，业务处理比较复杂，容易出错的系统（比如很多企业服务）适合本期的高性能模式。企业服务用户数量有限，但是稳定性要求高，单个用户异常可以接受，但是不允许单个用户异常影响到其他用户的使用。 多Reactor多进程/线程 日活跃1000w用户，则用户数肯定上亿了，论坛的业务复杂度不高，分库分表可以满足要求。假设每天每个用户查看100个页面，平均流量就是1000w*100/86400=1.15w qps/s,考虑峰值和负载余量，1.15 * 3 * 3=10w/s，考虑成本因素，由一台LVS在最前端做负载均衡，转发到多个集群上，每个集群使用nginx转发到具体机器上。 微信抢红包功能包括发红包，抢红包两个主要功能，发红包可以根据地理位置，响应时间等路由到某台服务器上去，但生成的id可以指示路由到该服务器，抢红包根据该id操作。所以抢红包的负载均衡算法是hash类的。 paxos算法大体是第一次由提交者Leader向所有其他服务器发出prepare消息请求准备，所有服务器中大多数如果回复诺言承诺就表示准备好了，可以接受写入；第二次提交者向所有服务器发出正式建议propose，所有服务器中大多数如果回复已经接收就表示成功了。 paxos在某些节点失效时仍能工作，所以其是AP架构，提供最终一致性。 不同的数据不同的设计。 商品数据库要保证可用性和最终一致性，因为用户最多的操作是浏览商品，这个体验要保证，有时候即使库存更新不及时也无所谓，很容易补救 秒杀的商品要保证一致性和原子性，因为秒杀出现秒杀成功而买不到商品时，用户体验很差 支付和账户余额的数据要保证强一致性和ACID，涉及到钱的数据很关键。 hdfs架构 mysql主从读写分离，写操作很少，读取比较多，架构简单，偶尔不可服务也是可以接受，只要能很快恢复就行。 远距离的分布集群受限于网络速度和网络不可靠因素，不能做到强一致性，如果一次写入需要客户等待几百毫秒以上，这个集群基本上就是不可用。 存储高可用复杂度远高于计算高可用，因为计算高可用基本是无状态的，只要保证服务可用就行，而存储是有状态的，除了可用还要保证正确。 不一定，有的异地多活是为了高可用，有的是为了业务。比如为了响应速度的异地多活，高可用架构是解决不了的。还有些异地多活是规避小概率事件，如地震，城市断电等，这也是高可用架构是解决不了的。 以数据说话，根据影响大小排序，在不做异地多活的情况下，影响多少用户，影响程度怎样等等 设计排队抢购功能。 面向功能拆分,规则引擎从结构上来看也属于微内核架构的一种具体实现 SOA是为企业客户设计解决历史遗留系统而设计的一种架构，互联网企业没有这个问题。 暂无 暂无 微服务基础设施包括自动化测试，自动化部署，配置中心，接口框架，api网关，服务发现，服务路由，服务容错，服务监控，服务跟踪，服务安全。 没有什么经验，参考github上spring-cloud-config，100多位贡献者，从开始到1.0release版用了1年多，考虑开源开发效率和后发优势，10位高级工程师开发一个类似spring-cloud-config的程序至少也需要2个月，按这个推算，开发整个基础设施需要2年时间。 Atlas 是一个 Android 客户端容器化框架，主要提供了组件化、动态性、解耦化的支持。 微内核设计关键点有：插件管理，插件连接和插件通信 Atlas的整体设计，分为五层： 第一层我们称之为Hack层，包括OS Hack toolkit & verifier，这里我们对系统能力做一些扩展，然后做一些安全校验。 第二层是Bundle Framework，就是我们的容器基础框架，提供Bundle管理、加载、生命周期、安全等一些最基本的能力。 第三层是运行期管理层，包括清单，我们会把所有的Bundle和它们的能力列在一个清单上，在调用时方便查找；另外是版本管理，会对所有Bundle的版本进行管理；再就是代理，这里就是和业界一些插件化框架机制类似的地方，我们会代理系统的运行环境，让Bundle运行在我们的容器框架上；然后还有调试和监控工具，是为了方便工程期开发调试。 第四层是业务层了，这里我们向业务方暴露了一些接口，如框架生命周期、配置文件、工具库等等。 最上面一层是应用接入层，就是我们的业务代码了。 看了一些android插件化的资料，这块知识非常缺少，对于altas的设计无法评价。 业务规模不一样导致选择的架构不一样。 在线广告，主要是商业模式的改变，开始是大流量平台包时段的广告位售卖模式，技术模式就是做一个广告发布平台，再就是搜索关键字广告平台，如Google的adwords，包含了很多小网站，模式更加复杂，需要大规模离线在线学习，然后出现了售卖长尾流量的rtb，实时性要求更高，系统更加复杂。 存储平台方案技术不是关键，使用现有各种开源技术就可以搭建存储平台，但是服务不是开源方案可以提供的，包括提供硬件，运维服务，安全服务等等，云计算产商提供了这些服务而成为存储平台方案。 一种开发框架和一种开发语言不是适合干所有事情的，可以选定一种开发框架和开发语言作为主要选择，在不适宜这种框架和语言的情况下也可以选择其他。 异地多活是和数据密切相关的，异地多活的是数据而不是系统，而且成本很高，只有核心数据核心业务才需要。 虚拟业务域划分就是为了解决子系统越来越多而提出的，所以粒度不应该太细，大概的数量在3-8个吧，太少无法体现划分，太多域和域的连接关系数太多。 按我的理解，中间件团队开发偏向技术，运维团队偏向业务，偏技术的自动化程度更高，偏业务的用户体验更好 需要 任何一个岗位都有营销的需求，都需要强有力的沟通 分步，按节点推进，先搭框架后优化 购买云服务，节省很多运维，安全的工作，另外云服务器已经提供数据安全备份，如果自己搭建开源系统，选择多份备份，会有极大浪费。 历史总是循环，组件化和容器化的app很重，未来网速够快，app就像一个网址，所有东西都是后端动态生成，如同现在的web一样。