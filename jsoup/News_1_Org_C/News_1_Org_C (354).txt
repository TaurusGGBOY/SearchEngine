该表持续不定期更新中！ Average Precision，就是对一个Query，计算其命中时的平均Precision。如何衡量一个模型的性能，单纯用 precision 和 recall 都不科学。于是人们想到，哎嘛为何不把 PR曲线下的面积 当做衡量尺度呢？于是就有了 AP值 这一概念。这里的 average，等于是在单个类别的总样本个数当中对 precision 进行 取平均 。 Batchsize的大小对训练过程中的资源要求有影响，较大的Batchsize会加快运算速度， 但是可能会导致内存不足，较小的Batchsize可以防止计算陷入局部最优。 ROC曲线下面的面积，用来评估模型。 假负类，在分类时，一个实例是正类，但是被预测为负类。 假正类，在分类时，一个实例是负类，但是被预测为正类。 FPR=FPFP+TR FPR=\frac{FP}{FP+TR} FPR=FP+TRFP F-Measure是Precision和Recall加权调和平均： F=(a2+1)P?Ra2(P+R) F = \frac{(a^2 + 1)P * R}{a^2(P + R)} F=a2(P+R)(a2+1)P?R 当参数a=1时， 就是最常见的F1了： F1=2PRP+R F1 = \frac{2PR}{P+R} F1=P+R2PR 很容易理解，F1综合了P和R的结果，当F1较高时则比较说明实验方法比较理想。 在目标检测中，IoU这一值，可以理解为系统预测出来的框与原来图片中标记的框的重合程度。计算方法即检测结果Detection Result与 Ground Truth 的交集比上它们的并集，即为检测的准确率：  学习率是一个很重要的超参数，过小的学习率会影响训练速度，过大的学习率会导致越过最优点。 学习率衰减，用来使学习率自适应调整大小。 Mean Average Precision，即 平均AP值 。mean表示不同的类别之间取平均。是对多个验证集个体 求 平均AP值 。如下图： mAP=1QR∑q∈QRAP(q) mAP = \frac{1}{Q_R}\sum_{q\in Q_R}AP(q) mAP=QR1q∈QR∑AP(q) Precision计算的是所有被检索到的item中，应该被检索到的item所占的比例，就是在识别出来的图片中，True positives所占的比例： precision=TPTP+FP=TPn precision = \frac{TP}{TP+FP}=\frac{TP}{n} precision=TP+FPTP=nTP 关于tp等参数，参考博客：https://blog.csdn.net/mdjxy63/article/details/79822555 Precision 精确率和Recall 召回率虽然没有必然的关系， 然而在大规模数据集合中，这两个指标确是相互制约的。 由于“检索策略”并不完美，希望更多相关的文档被检索到时，放宽“检索策略”时，往往也会伴随出现一些不相关的结果，从而使准确率受到影响。 而希望去除检索结果中的不相关文档时，务必要将“检索策略”定的更加严格，这样也会使有一些相关的文档不再能被检索到，从而使召回率受到影响。 Recall 是被正确识别出来的个数与测试集中所有该标签个数的比值： Recall=TPTP+FN Recall = \frac{TP}{TP+FN} Recall=TP+FNTP 横轴为 False Positvie Rate, 纵轴为 True Positive Rate, 的一条评估曲线。 可以通过调整阈值，来选择让系统识别出多少图片，进而改变Precision 或 Recall 的值。 真负类, 一个实例是负类, 被正确预测为负类。 真正类，一个实例是正类，被正确预测为正类。 TPR=TPTP+FN TPR=\frac{TP}{TP+FN} TPR=TP+FNTP 欢迎加入集美大学人工智能协会QQ群:283266234