在解决音视频播放同步前，有一些基本的知识点我需要说明一下。 上面一些音视频的基本知识点，是解决音视频播放同步的主要因素，因此必须先通过媒体文件，获取里面的音频与视频的信息，根据这些信息才能做好同步操作。那么如何获得这些信息呢？ 我们主要关注以下几点信息 文件时长：Duration: 00:14:10.67，此信息位于结构体AVFormatContext的duration成员，其实还可以获取其他信息例如bit_rate、packet_size 视频流：Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709), 1024x768, 808 kb/s, 8 fps, 8 tbr, 16 tbn, 16 tbc (default) 音频流：Stream #0:1(eng): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 317 kb/s (default) 那么这些数据都是从哪里得到的呢？ 在获得并根据多媒体文件更新一个AVFormatContext结构体变量之后，就可以在此结构的AVStream **streams成员中查找音视频流，并获得音视频流的各种信息 获取音频相关信息主要依靠struct AVCodecContext结构体，此结构体的变量位于AVStream结构中，当在AVFormatContext结构体的AVStream **streams成员中查找到音频流之后，就可以用以下方式获取音频信息： 获取视频相关信息与音频类似，当在AVFormatContext结构体的AVStream **streams成员中查找到视频流之后，就可以用以下方式获取视频信息： 通过以上的步骤，分别获取多媒体文件的音频与视频信息之后，就可以进行解码并播放。理论上只需要分别按照各自的时间要求播放音频与视频，他们本身应该就是同步的。假设一个多媒体文件的音频流为AAC编码，2声道，格式为16bit，采样率44.1KHz，视频流为H264编码，帧率为25fps，理论播放同步如下： 理论上只要按照上面的时间点，各自播放音频与视频，就可以同步了，但实际上，音频与视频播放都分别需要经过解码、重采样、播放3个步骤，每个步骤的耗时不一样，无法做到精确计时。 由此衍生出了3种同步的方法 ： 其实我更倾向于理论的方法，音频与视频各自播放互不打扰，从音视频播放的特点来说，人的听觉更为敏感，稍微的停顿都可以听出来，但是视觉就不一样了，人的视觉有暂留的效应； 因此根据理论的同步方式，对音频的播放不多加计算，尽快按照硬件所需数据的速度向硬件输入播放数据，又因为音频编解码的帧使用的解码时间戳DTS、播放时间戳PTS永远是一样的，因此只需要按照顺序进行解码播放即可 对于视频播放，由于H264编码的视频帧存在I帧、P帧、B帧，尤其是存在B帧的视频、其解码的顺序与播放顺序可能不一致，因此视频播放要先按解码顺序解码视频，然后按照音频播放的时间，在合适的时间点（PTS对应的时间）播放视频，由于不能精确计时，视频的早一点、迟一点，人的视觉几乎感觉不到，只要误差时间不超过视觉暂留的时间，并且误差不要累积；这实际上就是以音频为基准，视频向音频同步的过程 由以上分析可以看出，同步不是一次性完成的，而是时时刻刻在进行的，直到播放完毕。 关于DTS与PTS： 那么如何获取音视频的DTS与PTS呢？ 通过函数av_read_frame(pFormatCtx, Packet)读取一个AVPacket，在此结构中保存有每一帧的DTS、PTS信息 因为音频是顺序播放，因此音频中DTS和PTS是相同的。 视频中由于B帧需要双向预测，B帧依赖于其前和其后的帧，因此含B帧的视频解码顺序与显示顺序不同，即DTS与PTS不同；不含B帧的视频，其DTS和PTS是相同的。 以上部分把同步播放需要的信息，全都得到了，那么怎么实现音视频播放同步呢？很自然的我们需要多线程，不可能在一个线程里完成这些事情 通过以上的介绍，可以看出，我并没有刻意的使用将视频同步到音频，而是各自按照自己的速度去播放，貌似也还可以。下面就把代码贴上吧。